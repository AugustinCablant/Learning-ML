{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formation PYTORCH\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lien de la formation : \"https://automatants.cs-campus.fr/formation/2021_Intro_Pytorch\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch en quelques mots : <br>\n",
    "- Un backend pour le machine learning <br>\n",
    "- Importe des tenseurs (matrices) <br>\n",
    "- Permet de faire des descentes de gradient facilement <br>\n",
    "- Une librairie python <br>\n",
    "- Différences avec Tensorflow : PyTorch utilise les objets natifs de python et est séquentiel (point par point alors que Tensorflow est compilé), PyTorch est dynamique mais moins rapide, PyTorch est plus populaire chez les chercheurs, PyTorch ne dispose pas d'API de haut niveau. Les deux font la même chose ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas d'utilisation : MNIST \n",
    "Classification, sujet classique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Imports \n",
    "\n",
    "import torch \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "from torch import nn \n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=6.846699299663e-20, y = 6.846699299663e-20, \n",
      " res = 9.375457440488405e-39\n"
     ]
    }
   ],
   "source": [
    "## Backpropagation avec PyTorch \n",
    "\n",
    "x = torch.tensor(3.,requires_grad=True) # Track de gradient \n",
    "y = torch.tensor(3.,requires_grad=True)\n",
    "\n",
    "# On a nos paramètres x et y et on veut changer nos paramètres \n",
    "# de façon à minimiser la loss \n",
    "\n",
    "\n",
    "def f(x,y): \n",
    "    return x**2 + y**2 \n",
    "\n",
    "# On va chercher à minimiser cette fonction \n",
    "\n",
    "res = f(x,y)\n",
    "\n",
    "res.backward()\n",
    "\n",
    "### MAJ de x : descente de gradient\n",
    "# learning rate : \n",
    "lr = 1e-1\n",
    "\n",
    "# Opération qui n'est pas possible \n",
    "# x -= lr * x.grad\n",
    "# y -= lr * y.grad\n",
    "\n",
    "# Il faut saisir \n",
    "with torch.no_grad():\n",
    "    x -= lr * x.grad\n",
    "    y -= lr * y.grad\n",
    "\n",
    "for _ in range(200):\n",
    "    res = f(x,y) \n",
    "    res.backward()\n",
    "    with torch.no_grad():\n",
    "        x -= lr * x.grad\n",
    "        y -= lr * y.grad\n",
    "    x.grad.zero_() # remise à zéro du gradient qui est super importante sinon les gradients s'aditionnent \n",
    "    y.grad.zero_()\n",
    "\n",
    "print(f'x={x}, y = {y}, \\n res = {f(x,y)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
