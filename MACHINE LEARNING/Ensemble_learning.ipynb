{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning \n",
    "\n",
    "Combiner les résultats des différents modèles pour obtenir une meilleur performance : \"the wisdom of the crowd\" ! <br>\n",
    "<br>\n",
    "\n",
    "> \"La majorité dont chaque membre pris à part n'est pas un homme remarquable est cependant au-dessus des hommes supérieurs.\" \n",
    "\n",
    "**Aristote**, *La politique* <br>\n",
    "<br>\n",
    "C'est la loi faible des grands nombres ! <br>\n",
    "<br>\n",
    "Les modèles doivent : <br>\n",
    "- avoir au moins 50% de performance <br>\n",
    "- les modèles doivent présenter un minimum de diversité : ainsi les faiblesses des uns sont compensées par les forces des autres <br>\n",
    "<br>\n",
    "<br>\n",
    "Trois méthodes : <br>\n",
    "- Bagging <br>\n",
    "- Boosting <br>\n",
    "- Stacking "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Le bagging : \n",
    "\n",
    "Créer plusieurs entités d'un même modèle et entraîner chacune de ces unités sur une portion aléatoire de notre échantillon. Pour cela on utilise le bootstrapping (replacer après chaque tirage les données qui on été sélectionnées). Le plus connu est random forest (forêt aléaoitre !). <br>\n",
    "Dans le bagging chaque modèles est fort mais **overfit** son sub-set --> cela permet de <span style=\"color:red\">réduire la variance. </span>\n",
    "\n",
    "## Le boosting : \n",
    "\n",
    "Entraîner plusieurs modèles en demandant à chaque modèle de corriger les erreurs effectuées par son prédecesseur. Deux grands algo : AdaBoost et Gradient Boosting. Tous nos modèles sont en situation d'**underfitting** --> cela permet de <span style=\"color:red\">réduire le biais. </span>\n",
    "\n",
    "## Le stacking : \n",
    "\n",
    "Entraîner un modèle par dessus les prédictions de notre foule. On ne rassemble plus les résultats de notre modèle pour retenir une opinion majoritaire mais on demadne à notre modèle de reconnaître qui à tort ou raison dans notre foule pour qu'ils prédisent lui même le résultat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor, BaggingClassifier, BaggingRegressor, ExtraTreesClassifier, ExtraTreesRegressor, GradientBoostingClassifier, GradientBoostingRegressor, IsolationForest, RandomForestClassifier, RandomForestRegressor, RandomTreesEmbedding, StackingClassifier, StackingRegressor, VotingClassifier, VotingRegressor, HistGradientBoostingRegressor, HistGradientBoostingClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
